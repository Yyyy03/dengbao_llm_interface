{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "912cd8c6-d405-4dfe-8897-46108e6a6af7",
   "metadata": {},
   "source": [
    "# RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "id": "631b09a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T05:26:11.016771Z",
     "start_time": "2025-06-08T05:26:11.005771Z"
    }
   },
   "source": [
    "# NOTE: An OpenAI API key must be set here for application initialization, even if not in use.\n",
    "# If you're not utilizing OpenAI models, assign a placeholder string (e.g., \"not_used\").\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"fef51a476780489d8c1006f2fbd2f6\""
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "e2d7d995-7beb-40b5-9a44-afd350b7d221",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T06:49:46.913551Z",
     "start_time": "2025-06-08T06:49:46.904546Z"
    }
   },
   "source": [
    "# Cinderella story defined in sample.txt\n",
    "with open('demo/sample.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "print(text[:100])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The wife of a rich man fell sick, and as she felt that her end\n",
      "was drawing near, she called her only\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "c7d51ebd-5597-4fdd-8c37-32636395081b",
   "metadata": {},
   "source": [
    "1) **Building**: RAPTOR recursively embeds, clusters, and summarizes chunks of text to construct a tree with varying levels of summarization from the bottom up. You can create a tree from the text in 'sample.txt' using `RA.add_documents(text)`.\n",
    "\n",
    "2) **Querying**: At inference time, the RAPTOR model retrieves information from this tree, integrating data across lengthy documents at different abstraction levels. You can perform queries on the tree with `RA.answer_question`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f58830-9004-48a4-b50e-61a855511d24",
   "metadata": {},
   "source": [
    "### Building the tree"
   ]
  },
  {
   "cell_type": "code",
   "id": "3753fcf9-0a8e-4ab3-bf3a-6be38ef6cd1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T06:57:33.295370Z",
     "start_time": "2025-06-08T06:57:26.957580Z"
    }
   },
   "source": [
    "from raptor import RetrievalAugmentation "
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wyfan\\.conda\\envs\\raptor\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\wyfan\\.conda\\envs\\raptor\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "2025-06-08 14:57:32,959 - Loading faiss with AVX2 support.\n",
      "2025-06-08 14:57:32,977 - Successfully loaded faiss with AVX2 support.\n",
      "2025-06-08 14:57:32,982 - Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "7e843edf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T06:57:58.052615Z",
     "start_time": "2025-06-08T06:57:57.470271Z"
    }
   },
   "source": [
    "RA = RetrievalAugmentation()\n",
    "\n",
    "# construct the tree\n",
    "RA.add_documents(text)"
   ],
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOpenAIError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m RA \u001B[38;5;241m=\u001B[39m \u001B[43mRetrievalAugmentation\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# construct the tree\u001B[39;00m\n\u001B[0;32m      4\u001B[0m RA\u001B[38;5;241m.\u001B[39madd_documents(text)\n",
      "File \u001B[1;32mD:\\master\\llm\\raptor\\raptor\\RetrievalAugmentation.py:167\u001B[0m, in \u001B[0;36mRetrievalAugmentation.__init__\u001B[1;34m(self, config, tree)\u001B[0m\n\u001B[0;32m    160\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    161\u001B[0m \u001B[38;5;124;03mInitializes a RetrievalAugmentation instance with the specified configuration.\u001B[39;00m\n\u001B[0;32m    162\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m    163\u001B[0m \u001B[38;5;124;03m    config (RetrievalAugmentationConfig): The configuration for the RetrievalAugmentation instance.\u001B[39;00m\n\u001B[0;32m    164\u001B[0m \u001B[38;5;124;03m    tree: The tree instance or the path to a pickled tree file.\u001B[39;00m\n\u001B[0;32m    165\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    166\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m config \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 167\u001B[0m     config \u001B[38;5;241m=\u001B[39m \u001B[43mRetrievalAugmentationConfig\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    168\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(config, RetrievalAugmentationConfig):\n\u001B[0;32m    169\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    170\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconfig must be an instance of RetrievalAugmentationConfig\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    171\u001B[0m     )\n",
      "File \u001B[1;32mD:\\master\\llm\\raptor\\raptor\\RetrievalAugmentation.py:94\u001B[0m, in \u001B[0;36mRetrievalAugmentationConfig.__init__\u001B[1;34m(self, tree_builder_config, tree_retriever_config, qa_model, embedding_model, summarization_model, tree_builder_type, tr_tokenizer, tr_threshold, tr_top_k, tr_selection_mode, tr_context_embedding_model, tr_embedding_model, tr_num_layers, tr_start_layer, tb_tokenizer, tb_max_tokens, tb_num_layers, tb_threshold, tb_top_k, tb_selection_mode, tb_summarization_length, tb_summarization_model, tb_embedding_models, tb_cluster_embedding_model)\u001B[0m\n\u001B[0;32m     90\u001B[0m tree_builder_class, tree_builder_config_class \u001B[38;5;241m=\u001B[39m supported_tree_builders[\n\u001B[0;32m     91\u001B[0m     tree_builder_type\n\u001B[0;32m     92\u001B[0m ]\n\u001B[0;32m     93\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tree_builder_config \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 94\u001B[0m     tree_builder_config \u001B[38;5;241m=\u001B[39m \u001B[43mtree_builder_config_class\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     95\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtb_tokenizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     96\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtb_max_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     97\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_layers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtb_num_layers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     98\u001B[0m \u001B[43m        \u001B[49m\u001B[43mthreshold\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtb_threshold\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     99\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtop_k\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtb_top_k\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    100\u001B[0m \u001B[43m        \u001B[49m\u001B[43mselection_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtb_selection_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    101\u001B[0m \u001B[43m        \u001B[49m\u001B[43msummarization_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtb_summarization_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    102\u001B[0m \u001B[43m        \u001B[49m\u001B[43msummarization_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtb_summarization_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    103\u001B[0m \u001B[43m        \u001B[49m\u001B[43membedding_models\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtb_embedding_models\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    104\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcluster_embedding_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtb_cluster_embedding_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    105\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    107\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(tree_builder_config, tree_builder_config_class):\n\u001B[0;32m    108\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    109\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtree_builder_config must be a direct instance of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtree_builder_config_class\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for tree_builder_type \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtree_builder_type\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    110\u001B[0m     )\n",
      "File \u001B[1;32mD:\\master\\llm\\raptor\\raptor\\cluster_tree_builder.py:26\u001B[0m, in \u001B[0;36mClusterTreeConfig.__init__\u001B[1;34m(self, reduction_dimension, clustering_algorithm, clustering_params, *args, **kwargs)\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__init__\u001B[39m(\n\u001B[0;32m     19\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m     20\u001B[0m     reduction_dimension\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     24\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m     25\u001B[0m ):\n\u001B[1;32m---> 26\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     27\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreduction_dimension \u001B[38;5;241m=\u001B[39m reduction_dimension\n\u001B[0;32m     28\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclustering_algorithm \u001B[38;5;241m=\u001B[39m clustering_algorithm\n",
      "File \u001B[1;32mD:\\master\\llm\\raptor\\raptor\\tree_builder.py:85\u001B[0m, in \u001B[0;36mTreeBuilderConfig.__init__\u001B[1;34m(self, tokenizer, max_tokens, num_layers, threshold, top_k, selection_mode, summarization_length, summarization_model, embedding_models, cluster_embedding_model)\u001B[0m\n\u001B[0;32m     82\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msummarization_model \u001B[38;5;241m=\u001B[39m summarization_model\n\u001B[0;32m     84\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m embedding_models \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 85\u001B[0m     embedding_models \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOpenAI\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[43mOpenAIEmbeddingModel\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m}\n\u001B[0;32m     86\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(embedding_models, \u001B[38;5;28mdict\u001B[39m):\n\u001B[0;32m     87\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m     88\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124membedding_models must be a dictionary of model_name: instance pairs\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     89\u001B[0m     )\n",
      "File \u001B[1;32mD:\\master\\llm\\raptor\\raptor\\EmbeddingModels.py:19\u001B[0m, in \u001B[0;36mOpenAIEmbeddingModel.__init__\u001B[1;34m(self, model)\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, model\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext-embedding-ada-002\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m---> 19\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient \u001B[38;5;241m=\u001B[39m \u001B[43mOpenAI\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     20\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m=\u001B[39m model\n",
      "File \u001B[1;32m~\\.conda\\envs\\raptor\\lib\\site-packages\\openai\\_client.py:93\u001B[0m, in \u001B[0;36mOpenAI.__init__\u001B[1;34m(self, api_key, organization, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001B[0m\n\u001B[0;32m     91\u001B[0m     api_key \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39menviron\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOPENAI_API_KEY\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     92\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m api_key \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 93\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m OpenAIError(\n\u001B[0;32m     94\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     95\u001B[0m     )\n\u001B[0;32m     96\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapi_key \u001B[38;5;241m=\u001B[39m api_key\n\u001B[0;32m     98\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m organization \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mOpenAIError\u001B[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "f219d60a-1f0b-4cee-89eb-2ae026f13e63",
   "metadata": {},
   "source": [
    "### Querying from the tree\n",
    "\n",
    "```python\n",
    "question = # any question\n",
    "RA.answer_question(question)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4037c5-ad5a-424b-80e4-a67b8e00773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How did Cinderella reach her happy ending ?\"\n",
    "\n",
    "answer = RA.answer_question(question=question)\n",
    "\n",
    "print(\"Answer: \", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5be7e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tree by calling RA.save(\"path/to/save\")\n",
    "SAVE_PATH = \"demo/cinderella\"\n",
    "RA.save(SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e845de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load back the tree by passing it into RetrievalAugmentation\n",
    "\n",
    "RA = RetrievalAugmentation(tree=SAVE_PATH)\n",
    "\n",
    "answer = RA.answer_question(question=question)\n",
    "print(\"Answer: \", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277ab6ea-1c79-4ed1-97de-1c2e39d6db2e",
   "metadata": {},
   "source": [
    "## Using other Open Source Models for Summarization/QA/Embeddings\n",
    "\n",
    "If you want to use other models such as Llama or Mistral, you can very easily define your own models and use them with RAPTOR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86cbe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from raptor import BaseSummarizationModel, BaseQAModel, BaseEmbeddingModel, RetrievalAugmentationConfig\n",
    "from transformers import AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5cef43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to use the Gemma, you will need to authenticate with HuggingFace, Skip this step, if you have the model already downloaded\n",
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245b91a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "# You can define your own Summarization model by extending the base Summarization Class. \n",
    "class GEMMASummarizationModel(BaseSummarizationModel):\n",
    "    def __init__(self, model_name=\"google/gemma-2b-it\"):\n",
    "        # Initialize the tokenizer and the pipeline for the GEMMA model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.summarization_pipeline = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model_name,\n",
    "            model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "            device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),  # Use \"cpu\" if CUDA is not available\n",
    "        )\n",
    "\n",
    "    def summarize(self, context, max_tokens=150):\n",
    "        # Format the prompt for summarization\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": f\"Write a summary of the following, including as many key details as possible: {context}:\"}\n",
    "        ]\n",
    "        \n",
    "        prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        \n",
    "        # Generate the summary using the pipeline\n",
    "        outputs = self.summarization_pipeline(\n",
    "            prompt,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            top_p=0.95\n",
    "        )\n",
    "        \n",
    "        # Extracting and returning the generated summary\n",
    "        summary = outputs[0][\"generated_text\"].strip()\n",
    "        return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a171496d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GEMMAQAModel(BaseQAModel):\n",
    "    def __init__(self, model_name= \"google/gemma-2b-it\"):\n",
    "        # Initialize the tokenizer and the pipeline for the model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.qa_pipeline = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model_name,\n",
    "            model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "            device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "        )\n",
    "\n",
    "    def answer_question(self, context, question):\n",
    "        # Apply the chat template for the context and question\n",
    "        messages=[\n",
    "              {\"role\": \"user\", \"content\": f\"Given Context: {context} Give the best full answer amongst the option to question {question}\"}\n",
    "        ]\n",
    "        prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        \n",
    "        # Generate the answer using the pipeline\n",
    "        outputs = self.qa_pipeline(\n",
    "            prompt,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            top_p=0.95\n",
    "        )\n",
    "        \n",
    "        # Extracting and returning the generated answer\n",
    "        answer = outputs[0][\"generated_text\"][len(prompt):]\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878f7c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "class SBertEmbeddingModel(BaseEmbeddingModel):\n",
    "    def __init__(self, model_name=\"sentence-transformers/multi-qa-mpnet-base-cos-v1\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def create_embedding(self, text):\n",
    "        return self.model.encode(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255791ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAC = RetrievalAugmentationConfig(summarization_model=GEMMASummarizationModel(), qa_model=GEMMAQAModel(), embedding_model=SBertEmbeddingModel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee46f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "RA = RetrievalAugmentation(config=RAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe05daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('demo/sample.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "    \n",
    "RA.add_documents(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eee5847",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How did Cinderella reach her happy ending?\"\n",
    "\n",
    "answer = RA.answer_question(question=question)\n",
    "\n",
    "print(\"Answer: \", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAPTOR_env",
   "language": "python",
   "name": "raptor_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
